{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from skimage.transform import resize\n",
    "from tensorflow.keras import datasets, layers, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_factor = (188, 250)\n",
    "\n",
    "train_img_path = '../data/train_set/train_images/'\n",
    "test_img_path = '../data/train_set/test_images/'\n",
    "train_features_path = '../data/train_set/train_points/'\n",
    "test_features_path = '../data/train_set/test_points/'\n",
    "labels = pd.read_csv('../data/train_set/label.csv')\n",
    "\n",
    "train_img = sorted(os.listdir(train_img_path))\n",
    "test_img = sorted(os.listdir(test_img_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train emotion indexes\n",
    "train_indexes = []\n",
    "for img in train_img:\n",
    "    if img == \".DS_Store\":\n",
    "        continue\n",
    "    img = int(img[:4])\n",
    "    train_indexes.append(img - 1)\n",
    "    \n",
    "emotion_idx = labels[['emotion_idx']].loc[train_indexes].values[:, 0] - 1\n",
    "\n",
    "### Test emotion indexes\n",
    "test_indexes = []\n",
    "for img in test_img:\n",
    "    if img == \".DS_Store\":\n",
    "        continue\n",
    "    img = int(img[:4])\n",
    "    test_indexes.append(img - 1)\n",
    "    \n",
    "test_emotion_idx = labels[['emotion_idx']].loc[test_indexes].values[:, 0] - 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 77.71262311935425 seconds ---\n",
      "--- 20.534557104110718 seconds ---\n"
     ]
    }
   ],
   "source": [
    "imgs = []\n",
    "start_time = time.time()\n",
    "for img in train_img:\n",
    "    if img == '.DS_Store':\n",
    "        continue\n",
    "    img = np.asarray(Image.open(os.path.join(train_img_path, img)))\n",
    "    img = resize(img, resize_factor, anti_aliasing=False)\n",
    "    imgs.append(img)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))  \n",
    "\n",
    "imgs_test = []\n",
    "start_time = time.time()\n",
    "for img in test_img:\n",
    "    if img == '.DS_Store':\n",
    "        continue\n",
    "    img = np.asarray(Image.open(os.path.join('../data/train_set/test_images/', img)))\n",
    "    img = resize(img, resize_factor, anti_aliasing=False)\n",
    "    imgs_test.append(img)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1999 samples, validate on 501 samples\n",
      "Epoch 1/10\n",
      "1999/1999 [==============================] - 481s 241ms/step - loss: 3.1356 - acc: 0.0445 - val_loss: 3.0912 - val_acc: 0.0579\n",
      "Epoch 2/10\n",
      "1999/1999 [==============================] - 490s 245ms/step - loss: 3.0909 - acc: 0.0415 - val_loss: 3.0911 - val_acc: 0.0559\n",
      "Epoch 3/10\n",
      " 768/1999 [==========>...................] - ETA: 4:32 - loss: 3.0926 - acc: 0.0573"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-16-26618e42ffc2>\", line 13, in <module>\n",
      "    history = model.fit(np.array(imgs), emotion_idx, epochs=10, validation_data=(np.array(imgs_test), test_emotion_idx))\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 1363, in fit\n",
      "    validation_steps=validation_steps)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\", line 264, in fit_loop\n",
      "    outs = f(ins_batch)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\", line 2914, in __call__\n",
      "    fetched = self._callable_fn(*array_vals)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1382, in __call__\n",
      "    run_metadata_ptr)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 305, in wrapped\n",
      "    def wrapped(*args, **kwargs):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tornado.general:Uncaught exception in ZMQStream callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 421, in execute_request\n",
      "    self._abort_queues()\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 636, in _abort_queues\n",
      "    self._abort_queue(stream)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 642, in _abort_queue\n",
      "    idents,msg = self.session.recv(stream, zmq.NOBLOCK, content=True)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/jupyter_client/session.py\", line 815, in recv\n",
      "    return idents, self.deserialize(msg_list, content=content, copy=copy)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/jupyter_client/session.py\", line 930, in deserialize\n",
      "    message['parent_header'] = extract_dates(self.unpack(msg_list[2]))\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/jupyter_client/jsonutil.py\", line 66, in extract_dates\n",
      "    for k,v in iteritems(obj):\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/ipython_genutils/py3compat.py\", line 188, in iteritems\n",
      "    def iteritems(d): return iter(d.items())\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu', input_shape=(188, 250, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(22, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(np.array(imgs), emotion_idx, epochs=10, validation_data=(np.array(imgs_test), test_emotion_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN with Xception pretrained model:\n",
    "In this model we use a pretrained model to find the fteatures, and then add a 3 layer CNN on top of that model as a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "import tensorflow.contrib.eager as tfe\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XceptionBottleneck(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(XceptionBottleneck, self).__init__()\n",
    "        \n",
    "        # Define xception layer (include_top=False and use imagenet weights), \n",
    "        # see: https://keras.io/applications/#models-for-image-classification-with-weights-trained-on-imagenet\n",
    "        self.xception_layers = Xception(weights='imagenet', include_top=False)\n",
    "        \n",
    "        # Define pooling GlobalAveragePooling2D \n",
    "        # see: https://keras.io/layers/pooling/  \n",
    "        self.pooling_layer = GlobalAveragePooling2D()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        result = self.xception_layers(inputs)\n",
    "        result = self.pooling_layer(result)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_fmr_images(file_paths):\n",
    "    # Reads an image from a file, decodes it into a dense tensor, and resizes it\n",
    "    # to a fixed shape. Read more here: https://www.tensorflow.org/guide/datasets#decoding_image_data_and_resizing_it\n",
    "    def _parse_function(filename):\n",
    "        image_string = tf.read_file(filename)\n",
    "        image_decoded = tf.image.decode_png(image_string)\n",
    "        image_resized = tf.image.resize_images(image_decoded, [200, 200])\n",
    "        return image_resized\n",
    "\n",
    "    file_paths = tf.constant(file_paths)\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((file_paths))\n",
    "    dataset = dataset.map(_parse_function)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def cache_bottleneck_layers(file_paths, batch_size, device):\n",
    "    \n",
    "    bottle_necks = []\n",
    "    dataset = create_dataset_fmr_images(file_paths).batch(batch_size)\n",
    "    n_samples = len(file_paths)\n",
    "\n",
    "    device = \"gpu:0\" if tfe.num_gpus() else \"cpu:0\"\n",
    "    \n",
    "    with tf.device(device):\n",
    "        xception_out = XceptionBottleneck()\n",
    "        for batch_num, image in enumerate(dataset):\n",
    "            print('\\rComputing bottle neck layers... batch {} of {}'.format(batch_num+1, n_samples//batch_size), end=\"\")\n",
    "            \n",
    "            # Compute bottle necks layer for image batch convert to numpy and append to bottle_necks\n",
    "            # ...\n",
    "            # ...\n",
    "            result = xception_out(image)\n",
    "            result = result.numpy()\n",
    "            bottle_necks.append(result)\n",
    "            \n",
    "    return np.vstack(bottle_necks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_path = '../data/train_set/train_images/'\n",
    "train_img = sorted(os.listdir(train_img_path))\n",
    "train_indexes = []\n",
    "for img in train_img:\n",
    "    if img == \".DS_Store\":\n",
    "        continue\n",
    "    img = int(img[:4])\n",
    "    train_indexes.append(img - 1)\n",
    "    \n",
    "emotion_idx = labels[['emotion_idx']].loc[train_indexes].values[:, 0] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_paths = []\n",
    "for img in train_img:\n",
    "    if img == \".DS_Store\":\n",
    "        continue\n",
    "    train_file_paths.append(os.path.join(train_img_path, img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing bottle neck layers... batch 40 of 39--- 864.5508890151978 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "device = \"gpu:0\" if tfe.num_gpus() else \"cpu:0\"\n",
    "bottle_necks = cache_bottleneck_layers(train_file_paths, batch_size=50, device=device)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = \"./cache/\"\n",
    "fname = 'bottle_neck.npz'\n",
    "save_path = os.path.join(cache_path,fname)\n",
    "\n",
    "if not os.path.isdir(cache_path): \n",
    "    os.mkdir(cache_path)\n",
    "    \n",
    "np.savez(save_path, bottle_necks=bottle_necks, labels=emotion_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XceptionClassifier(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, n_classes):\n",
    "        super(XceptionClassifier, self).__init__()\n",
    "        # Define the layer(s) you would like to use for your classifier\n",
    "        self.dense_layer_1 = tf.keras.layers.Dense(units=20*n_classes, activation=tf.keras.activations.relu)\n",
    "        self.dense_layer_2 = tf.keras.layers.Dense(units=5*n_classes, activation=tf.keras.activations.relu)\n",
    "        self.dense_layer_3 = tf.keras.layers.Dense(units=n_classes)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Set this up appropriately, will depend on how many layers you choose\n",
    "        result = self.dense_layer_1(inputs)\n",
    "        result = self.dense_layer_2(result)\n",
    "        result = self.dense_layer_3(result)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this once for all models, in order to have same shuffeled data_set for all of them.\n",
    "data = np.load(save_path)\n",
    "train_bottle_necks, train_labels = data['bottle_necks'],  data['labels']\n",
    "n_train_samples= len(train_bottle_necks)\n",
    "\n",
    "# zip the index of images with their indexes\n",
    "train_bottle_necks = list(zip(range(n_train_samples), train_bottle_necks))\n",
    "\n",
    "# splitting the data to three different part. 70% for train the model, 15% for the validation modeling, and 15% for\n",
    "# trian modeling\n",
    "\n",
    "train_bottle_necks, X_test, train_labels, Y_test = train_test_split(train_bottle_necks, train_labels, test_size=0.3, random_state=40)\n",
    "X_test, X_val, Y_test, Y_val = train_test_split(X_test, Y_test, test_size=0.5, random_state=40)\n",
    "\n",
    "# unzip the train_bottle_necks\n",
    "_, train_bottle_necks = zip(*train_bottle_necks)\n",
    "train_bottle_necks = np.array(train_bottle_necks)\n",
    "\n",
    "_, X_val = zip(*X_val)\n",
    "X_val = np.array(X_val)\n",
    "\n",
    "index_test, X_test = zip(*X_test)\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16        # You will play around with this \n",
    "n_epochs = 200         # Choose num epochs based on when you think the parameters have converged\n",
    "learning_rate = 0.00001 # You will try different learning rates\n",
    "\n",
    "train_loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_dataset = tf.data.Dataset.from_tensor_slices(train_bottle_necks)\n",
    "train_labels_dataset = tf.data.Dataset.from_tensor_slices(train_labels)\n",
    "train_dataset = tf.data.Dataset.zip((train_images_dataset, train_labels_dataset))\n",
    "train_dataset = train_dataset.batch(batch_size) \n",
    "# since in train_test split the shuffle is True, and having the same train data set for all models, I do not call\n",
    "# 'shuffle(buffer_size=n_train_samples)' here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_classifier = XceptionClassifier(n_classes=22)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 199, Batch: 80, Loss: 1.84995436668396--- 96.57389521598816 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "with tf.device(device):\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch, (images, labels) in enumerate(train_dataset):\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Compute logits\n",
    "                logits = x_classifier(images)\n",
    "                xe_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
    "                \n",
    "            train_loss_history.append(xe_loss.numpy())\n",
    "            # Compute gradient and apply gradients\n",
    "            grads = tape.gradient(xe_loss, x_classifier.variables)\n",
    "            optimizer.apply_gradients(zip(grads, x_classifier.variables),\n",
    "                                      global_step=tf.train.get_or_create_global_step())\n",
    "            \n",
    "            if batch % 10 == 0:\n",
    "                print('\\rEpoch: {}, Batch: {}, Loss: {}'.format(epoch, batch, train_loss_history[-1]), end='')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3909935668334525 0.043333333333333335\n"
     ]
    }
   ],
   "source": [
    "## computing the train accuracy for first model\n",
    "logits = x_classifier(tf.constant(train_bottle_necks))\n",
    "y_pred = tf.nn.softmax(logits).numpy()\n",
    "train_pred = np.argmax(y_pred, axis=1)\n",
    "train_accuracy = np.sum(train_pred == train_labels) / len(train_pred)\n",
    "\n",
    "## computing the validation accuracy for first model\n",
    "logits = x_classifier(tf.constant(X_val))\n",
    "y_pred = tf.nn.softmax(logits).numpy()\n",
    "val_pred = np.argmax(y_pred, axis=1)\n",
    "val_accuracy = np.sum(val_pred == Y_val) / len(val_pred)\n",
    "\n",
    "print(train_accuracy, val_accuracy)\n",
    "# .... complete parts (e,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (BMEN4000)",
   "language": "python",
   "name": "bmen4000"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
