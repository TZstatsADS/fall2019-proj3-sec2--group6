{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO:\n",
    "\n",
    "- correct the path for the pipeline\n",
    "- correct the pathes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.feature import hog\n",
    "from skimage.transform import resize\n",
    "from skimage import data, exposure\n",
    "from PIL import Image\n",
    "from sklearn.svm import SVC #SVM classifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from skimage.feature import local_binary_pattern\n",
    "from skimage.color import rgb2gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_and_flatten(features):\n",
    "    features = features - np.min(features, axis=0, keepdims=True)\n",
    "    features = features / np.max(features, axis=0, keepdims=True)\n",
    "    return features.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_factor = (188, 250)\n",
    "\n",
    "train_img_path = '../data/train_set/train_images/'\n",
    "test_img_path = '../data/train_set/test_images/'\n",
    "train_features_path = '../data/train_set/train_points/'\n",
    "test_features_path = '../data/train_set/test_points/'\n",
    "labels = pd.read_csv('../data/train_set/label.csv')\n",
    "\n",
    "train_img = sorted(os.listdir(train_img_path))\n",
    "test_img = sorted(os.listdir(test_img_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRUE Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train emotion indexes\n",
    "train_indexes = []\n",
    "for img in train_img:\n",
    "    if img == \".DS_Store\":\n",
    "        continue\n",
    "    img = int(img[:4])\n",
    "    train_indexes.append(img - 1)\n",
    "    \n",
    "emotion_idx = labels[['emotion_idx']].loc[train_indexes].values[:, 0]\n",
    "\n",
    "### Test emotion indexes\n",
    "test_indexes = []\n",
    "for img in test_img:\n",
    "    if img == \".DS_Store\":\n",
    "        continue\n",
    "    img = int(img[:4])\n",
    "    test_indexes.append(img - 1)\n",
    "    \n",
    "test_emotion_idx = labels[['emotion_idx']].loc[test_indexes].values[:, 0]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEATURES\n",
    "#### HOGS, train_features, concat_train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 124.47650098800659 seconds ---\n",
      "--- 30.952392101287842 seconds ---\n"
     ]
    }
   ],
   "source": [
    "### extract HOG for train data set\n",
    "\n",
    "HOGs = []\n",
    "start_time = time.time()\n",
    "for img in train_img:\n",
    "    if img == '.DS_Store':\n",
    "        continue\n",
    "    img = np.asarray(Image.open(os.path.join(train_img_path, img)))\n",
    "    img = resize(img, resize_factor, anti_aliasing=False)\n",
    "    fetures_hog = hog(img, orientations=9, pixels_per_cell=(8, 8),\n",
    "                    cells_per_block=(3, 3), visualize=False, multichannel=True)\n",
    "    HOGs.append(fetures_hog)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))  \n",
    "\n",
    "### extract HOG for test data set\n",
    "\n",
    "HOGs_test = []\n",
    "start_time = time.time()\n",
    "for img in test_img:\n",
    "    if img == '.DS_Store':\n",
    "        continue\n",
    "    img = np.asarray(Image.open(os.path.join('../data/train_set/test_images/', img)))\n",
    "    img = resize(img, resize_factor, anti_aliasing=False)\n",
    "    fetures_hog = hog(img, orientations=9, pixels_per_cell=(8, 8),\n",
    "                    cells_per_block=(3, 3), visualize=False, multichannel=True)\n",
    "    HOGs_test.append(fetures_hog)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Point features for train dataset (NORMALIZED VERSION)\n",
    "train_features = sorted(os.listdir(train_features_path))\n",
    "train_features_list = []\n",
    "for f in train_features:\n",
    "    tmp = sio.loadmat(os.path.join(train_features_path, f))\n",
    "    if 'faceCoordinatesUnwarped' in tmp:\n",
    "        features = tmp['faceCoordinatesUnwarped']\n",
    "    else:\n",
    "        features = tmp['faceCoordinates2']\n",
    "    train_features_list.append(normalize_and_flatten(features))\n",
    "    \n",
    "### Point features for test dataset (NORMALIZED VERSION)\n",
    "test_features = sorted(os.listdir(test_features_path))\n",
    "test_features_list = []\n",
    "for f in test_features:\n",
    "    tmp = sio.loadmat(os.path.join(test_features_path, f))\n",
    "    if 'faceCoordinatesUnwarped' in tmp:\n",
    "        features = tmp['faceCoordinatesUnwarped']\n",
    "    else:\n",
    "        features = tmp['faceCoordinates2']\n",
    "    test_features_list.append(normalize_and_flatten(features))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features_list[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HOGs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-9858e882ee76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mreduced_hogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHOGs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtrain_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mconcat_train_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduced_hogs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HOGs' is not defined"
     ]
    }
   ],
   "source": [
    "### Cancate of HOG and .mat features train\n",
    "start_time = time.time()\n",
    "pca = PCA(n_components=100)\n",
    "reduced_hogs = pca.fit_transform(np.array(HOGs)) \n",
    "train_np = np.array(train_features_list)\n",
    "concat_train_features = np.concatenate((reduced_hogs, train_np), axis=1)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "### test\n",
    "start_time = time.time()\n",
    "reduced_hogs_test = pca.transform(np.array(HOGs_test))\n",
    "test_np = np.array(test_features_list)\n",
    "concat_test_features = np.concatenate((reduced_hogs_test, test_np), axis=1)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 103.11895799636841 seconds ---\n",
      "--- 26.66599416732788 seconds ---\n"
     ]
    }
   ],
   "source": [
    "### extract LBP for train data set\n",
    "\n",
    "METHOD = 'uniform'\n",
    "radius = 3\n",
    "n_points = 8 * radius\n",
    "\n",
    "LBPs = []\n",
    "start_time = time.time()\n",
    "for img in train_img:\n",
    "    if img == '.DS_Store':\n",
    "        continue\n",
    "    img = np.asarray(Image.open(os.path.join(train_img_path, img)))\n",
    "    img = resize(img, resize_factor, anti_aliasing=False)\n",
    "    img = rgb2gray(img)\n",
    "#     print(img.shape)    \n",
    "    fetures_lbp = local_binary_pattern(img, n_points, radius, METHOD)\n",
    "    LBPs.append(fetures_lbp.flatten())\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))  \n",
    "\n",
    "### extract LBP for test data set\n",
    "\n",
    "LBPs_test = []\n",
    "start_time = time.time()\n",
    "for img in test_img:\n",
    "    if img == '.DS_Store':\n",
    "        continue\n",
    "    img = np.asarray(Image.open(os.path.join('../data/train_set/test_images/', img)))\n",
    "    img = resize(img, resize_factor, anti_aliasing=False)\n",
    "    img = rgb2gray(img)\n",
    "    fetures_lbp = local_binary_pattern(img, n_points, radius, METHOD)\n",
    "    LBPs_test.append(fetures_lbp.flatten())\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASSIFIERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVM on HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Duration : 482.69038796424866 seconds\n",
      "Train Accuracy Durtion: 234.04115295410156 seconds\n",
      "Train Accuracy: 0.09404702351175588\n",
      "Test Accuracy Duration: 60.57271194458008 seconds\n",
      "Test Accuracy: 0.06786427145708583\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "linear_SVM = SVC(kernel=\"linear\", C=0.001)\n",
    "linear_SVM.fit(np.array(HOGs), emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = linear_SVM.score(np.array(HOGs), emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = linear_SVM.score(np.array(HOGs_test), test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVM on LBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Duration : 473.7649929523468 seconds\n",
      "Train Accuracy Durtion: 236.37778902053833 seconds\n",
      "Train Accuracy: 0.9994997498749375\n",
      "Test Accuracy Duration: 56.19797611236572 seconds\n",
      "Test Accuracy: 0.02594810379241517\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "linear_SVM = SVC(kernel=\"linear\", C=0.001)\n",
    "linear_SVM.fit(np.array(LBPs), emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = linear_SVM.score(np.array(LBPs), emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = linear_SVM.score(np.array(LBPs_test), test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVM on PCA on LBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Duration : 12.468503713607788 seconds\n",
      "Train Accuracy Durtion: 0.9384250640869141 seconds\n",
      "Train Accuracy: 0.9994997498749375\n",
      "Test Accuracy Duration: 0.6064751148223877 seconds\n",
      "Test Accuracy: 0.041916167664670656\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pca = PCA(n_components=200)\n",
    "reduced_features = pca.fit_transform(np.array(LBPs))\n",
    "linear_SVM = SVC(kernel=\"linear\", C=0.0001)\n",
    "linear_SVM.fit(reduced_features, emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = linear_SVM.score(reduced_features, emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "reduced_test = pca.transform(np.array(LBPs_test))\n",
    "accuracy = linear_SVM.score(reduced_test, test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVM on .mat features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Duration : 0.6397969722747803 seconds\n",
      "Train Accuracy Durtion: 0.7331209182739258 seconds\n",
      "Train Accuracy: 0.7928964482241121\n",
      "Test Accuracy Duration: 0.17191791534423828 seconds\n",
      "Test Accuracy: 0.4630738522954092\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "linear_SVM = SVC(kernel=\"linear\", C=100)\n",
    "linear_SVM.fit(np.array(train_features_list), emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = linear_SVM.score(np.array(train_features_list), emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = linear_SVM.score(np.array(test_features_list), test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L SVM on concat features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Duration : 2.706681966781616 seconds\n",
      "Train Accuracy Durtion: 1.2310597896575928 seconds\n",
      "Train Accuracy: 0.7898949474737369\n",
      "Test Accuracy Duration: 0.32773709297180176 seconds\n",
      "Test Accuracy: 0.11377245508982035\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "linear_SVM = SVC(kernel=\"linear\", C=0.5)\n",
    "linear_SVM.fit(concat_train_features, emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = linear_SVM.score(concat_train_features, emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = linear_SVM.score(concat_test_features, test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L SVM on PCA HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# pca = PCA(n_components=200)\n",
    "# reduced_features = pca.fit_transform(np.array(HOGs))\n",
    "linear_SVM = SVC(kernel=\"linear\", C=1)\n",
    "linear_SVM.fit(reduced_features, emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = linear_SVM.score(reduced_features, emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "reduced_test = pca.transform(np.array(HOGs_test))\n",
    "accuracy = linear_SVM.score(reduced_test, test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM on PCA HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Duration : 15.348134279251099 seconds\n",
      "Train Accuracy Durtion: 0.5401968955993652 seconds\n",
      "Train Accuracy: 0.9934967483741871\n",
      "Test Accuracy Duration: 0.5810391902923584 seconds\n",
      "Test Accuracy: 0.04590818363273453\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pca = PCA(n_components=100)\n",
    "reduced_features = pca.fit_transform(np.array(HOGs))\n",
    "model = SVC(gamma='auto', C=10)\n",
    "model.fit(reduced_features, emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(reduced_features, emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "reduced_test = pca.transform(np.array(HOGs_test))\n",
    "accuracy = model.score(reduced_test, test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Duration : 11.554597854614258 seconds\n",
      "Train Accuracy Durtion: 0.9510560035705566 seconds\n",
      "Train Accuracy: 0.05202601300650325\n",
      "Test Accuracy Duration: 0.666874885559082 seconds\n",
      "Test Accuracy: 0.06187624750499002\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pca = PCA(n_components=200)\n",
    "reduced_features = pca.fit_transform(np.array(LBPs))\n",
    "model = SVC(gamma='auto', C=0.01)\n",
    "model.fit(reduced_features, emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(reduced_features, emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "reduced_test = pca.transform(np.array(LBPs_test))\n",
    "accuracy = model.score(reduced_test, test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM on .mat Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Duration : 0.7238380908966064 seconds\n",
      "Train Accuracy Durtion: 0.8284599781036377 seconds\n",
      "Train Accuracy: 0.6183091545772886\n",
      "Test Accuracy Duration: 0.1888411045074463 seconds\n",
      "Test Accuracy: 0.46506986027944114\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model = SVC(gamma='auto', C=1000)\n",
    "model.fit(np.array(train_features_list), emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(np.array(train_features_list), emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(np.array(test_features_list), test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM on concat features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Duration : 2.657891035079956 seconds\n",
      "Train Accuracy Durtion: 1.3531501293182373 seconds\n",
      "Train Accuracy: 0.8539269634817409\n",
      "Test Accuracy Duration: 0.36635494232177734 seconds\n",
      "Test Accuracy: 0.07584830339321358\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model = SVC(gamma='auto', C=10)\n",
    "model.fit(concat_train_features, emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(concat_train_features, emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(concat_test_features, test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForestClassifier on PCA HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Duration : 11.663228273391724 seconds\n",
      "Train Accuracy Durtion: 0.02125692367553711 seconds\n",
      "Train Accuracy: 0.5317658829414708\n",
      "Test Accuracy Duration: 0.337507963180542 seconds\n",
      "Test Accuracy: 0.06187624750499002\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pca = PCA(n_components=100)\n",
    "reduced_features = pca.fit_transform(np.array(HOGs))\n",
    "model = RandomForestClassifier(max_depth=5, n_estimators=50)\n",
    "model.fit(reduced_features, emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(reduced_features, emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "reduced_test = pca.transform(np.array(HOGs_test))\n",
    "accuracy = model.score(reduced_test, test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Duration : 16.7584331035614 seconds\n",
      "Train Accuracy Durtion: 0.03023386001586914 seconds\n",
      "Train Accuracy: 0.6543271635817909\n",
      "Test Accuracy Duration: 0.48764681816101074 seconds\n",
      "Test Accuracy: 0.05788423153692615\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pca = PCA(n_components=200)\n",
    "reduced_features = pca.fit_transform(np.array(LBPs))\n",
    "model = RandomForestClassifier(max_depth=5, n_estimators=100)\n",
    "model.fit(reduced_features, emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(reduced_features, emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "reduced_test = pca.transform(np.array(LBPs_test))\n",
    "accuracy = model.score(reduced_test, test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RFC on .mat features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Duration : 1.1041409969329834 seconds\n",
      "Train Accuracy Durtion: 0.030183076858520508 seconds\n",
      "Train Accuracy: 0.5157578789394698\n",
      "Test Accuracy Duration: 0.011514902114868164 seconds\n",
      "Test Accuracy: 0.3013972055888224\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model = RandomForestClassifier(max_depth=5, n_estimators=100)\n",
    "model.fit(np.array(train_features_list), emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(np.array(train_features_list), emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(np.array(test_features_list), test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RFC on cancate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Duration : 1.3742477893829346 seconds\n",
      "Train Accuracy Durtion: 0.05086398124694824 seconds\n",
      "Train Accuracy: 0.5327663831915957\n",
      "Test Accuracy Duration: 0.025084972381591797 seconds\n",
      "Test Accuracy: 0.3073852295409182\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model = RandomForestClassifier(max_depth=5, n_estimators=100)\n",
    "model.fit(concat_train_features, emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(concat_train_features, emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(concat_test_features, test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLPClassifier on pca HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Duration : 19.290612936019897 seconds\n",
      "Train Accuracy Durtion: 0.04608917236328125 seconds\n",
      "Train Accuracy: 0.9904952476238119\n",
      "Test Accuracy Duration: 0.9957513809204102 seconds\n",
      "Test Accuracy: 0.08183632734530938\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pca = PCA(n_components=100)\n",
    "reduced_features = pca.fit_transform(np.array(HOGs))\n",
    "model = MLPClassifier(hidden_layer_sizes=(300, 600, 300, 150, 60, 40))\n",
    "model.fit(reduced_features, emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(reduced_features, emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "reduced_test = pca.transform(np.array(HOGs_test))\n",
    "accuracy = model.score(reduced_test, test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Duration : 19.16475510597229 seconds\n",
      "Train Accuracy Durtion: 0.056764841079711914 seconds\n",
      "Train Accuracy: 0.9984992496248124\n",
      "Test Accuracy Duration: 0.4417610168457031 seconds\n",
      "Test Accuracy: 0.05189620758483034\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pca = PCA(n_components=200)\n",
    "reduced_features = pca.fit_transform(np.array(LBPs))\n",
    "model = MLPClassifier(hidden_layer_sizes=(300, 600, 300, 150, 60, 40))\n",
    "model.fit(reduced_features, emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(reduced_features, emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "reduced_test = pca.transform(np.array(LBPs_test))\n",
    "accuracy = model.score(reduced_test, test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP on .mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Duration : 3.587941884994507 seconds\n",
      "Train Accuracy Durtion: 0.0499267578125 seconds\n",
      "Train Accuracy: 0.1265632816408204\n",
      "Test Accuracy Duration: 0.011600971221923828 seconds\n",
      "Test Accuracy: 0.10778443113772455\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model = MLPClassifier(hidden_layer_sizes=(300, 600, 300, 150, 60, 40))\n",
    "model.fit(np.array(train_features_list), emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(np.array(train_features_list), emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(np.array(test_features_list), test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP on concat features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Duration : 5.981200933456421 seconds\n",
      "Train Accuracy Durtion: 0.04111790657043457 seconds\n",
      "Train Accuracy: 0.9959979989994997\n",
      "Test Accuracy Duration: 0.010310888290405273 seconds\n",
      "Test Accuracy: 0.11976047904191617\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model = MLPClassifier(hidden_layer_sizes=(300, 600, 300, 60, 40))\n",
    "model.fit(concat_train_features, emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(concat_train_features, emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(concat_test_features, test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN on .mat features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Duration : 0.008569002151489258 seconds\n",
      "Train Accuracy Durtion: 1.2204737663269043 seconds\n",
      "Train Accuracy: 0.4497248624312156\n",
      "Test Accuracy Duration: 0.288834810256958 seconds\n",
      "Test Accuracy: 0.22355289421157684\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "model.fit(np.array(train_features_list), emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(np.array(train_features_list), emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(np.array(test_features_list), test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN on pca HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Duration : 14.751265048980713 seconds\n",
      "Train Accuracy Durtion: 1.218137264251709 seconds\n",
      "Train Accuracy: 0.1905952976488244\n",
      "Test Accuracy Duration: 0.602686882019043 seconds\n",
      "Test Accuracy: 0.029940119760479042\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pca = PCA(n_components=100)\n",
    "reduced_features = pca.fit_transform(np.array(HOGs))\n",
    "model = KNeighborsClassifier(n_neighbors=10)\n",
    "model.fit(reduced_features, emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(reduced_features, emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "reduced_test = pca.transform(np.array(HOGs_test))\n",
    "accuracy = model.score(reduced_test, test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN on concate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Duration : 0.02538895606994629 seconds\n",
      "Train Accuracy Durtion: 1.2404861450195312 seconds\n",
      "Train Accuracy: 0.3476738369184592\n",
      "Test Accuracy Duration: 0.32026219367980957 seconds\n",
      "Test Accuracy: 0.015968063872255488\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "model.fit(concat_train_features, emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(concat_train_features, emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(concat_test_features, test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Duration : 23.674659967422485 seconds\n",
      "[0.18382353 0.15880893 0.12219451 0.13636364 0.10741688]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "start_time = time.time()\n",
    "model = AdaBoostClassifier(n_estimators=150)\n",
    "scores = cross_val_score(model, np.array(train_features_list), emotion_idx, cv=5)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "print(scores)\n",
    "\n",
    "# model.fit(np.array(train_features_list), emotion_idx)\n",
    "# print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "# start_time = time.time()\n",
    "# accuracy = model.score(np.array(train_features_list), emotion_idx)\n",
    "# print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "# print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "# start_time = time.time()\n",
    "# accuracy = model.score(np.array(test_features_list), test_emotion_idx)\n",
    "# print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "# print(\"Test Accuracy: {}\".format(accurac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1999, 156)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import rotate\n",
    "from skimage.feature import local_binary_pattern\n",
    "from skimage import data\n",
    "from skimage.color import label2rgb\n",
    "\n",
    "# settings for LBP\n",
    "radius = 3\n",
    "n_points = 8 * radius\n",
    "\n",
    "\n",
    "def overlay_labels(image, lbp, labels):\n",
    "    mask = np.logical_or.reduce([lbp == each for each in labels])\n",
    "    return label2rgb(mask, image=image, bg_label=0, alpha=0.5)\n",
    "\n",
    "\n",
    "def highlight_bars(bars, indexes):\n",
    "    for i in indexes:\n",
    "        bars[i].set_facecolor('r')\n",
    "\n",
    "\n",
    "image = data.brick()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 512)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Duration : 0.05864381790161133 seconds\n",
      "Train Accuracy Durtion: 0.004312276840209961 seconds\n",
      "Train Accuracy: 0.7563781890945472\n",
      "Test Accuracy Duration: 0.0032968521118164062 seconds\n",
      "Test Accuracy: 0.5508982035928144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "start_time = time.time()\n",
    "model = LDA()\n",
    "model.fit(np.array(train_features_list), emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(np.array(train_features_list), emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(np.array(test_features_list), test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Duration : 15.780615091323853 seconds\n",
      "Train Accuracy Durtion: 0.003587007522583008 seconds\n",
      "Train Accuracy: 0.3871935967983992\n",
      "Test Accuracy Duration: 0.7522678375244141 seconds\n",
      "Test Accuracy: 0.03592814371257485\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pca = PCA(n_components=200)\n",
    "reduced_features = pca.fit_transform(np.array(LBPs))\n",
    "model = LDA()\n",
    "model.fit(reduced_features, emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(reduced_features, emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "reduced_test = pca.transform(np.array(LBPs_test))\n",
    "accuracy = model.score(reduced_test, test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.19.1'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "### calculating the distance\n",
    "def normalize(features):\n",
    "    features = features - np.min(features, axis=0, keepdims=True)\n",
    "    features = features / np.max(features, axis=0, keepdims=True)\n",
    "    return features\n",
    "\n",
    "train_features = sorted(os.listdir(train_features_path))\n",
    "train_features_list2 = []\n",
    "for f in train_features:\n",
    "    tmp = sio.loadmat(os.path.join(train_features_path, f))\n",
    "    if 'faceCoordinatesUnwarped' in tmp:\n",
    "        features = tmp['faceCoordinatesUnwarped']\n",
    "    else:\n",
    "        features = tmp['faceCoordinates2']\n",
    "    train_features_list2.append(normalize(features))\n",
    "    \n",
    "test_features = sorted(os.listdir(test_features_path))\n",
    "test_features_list2 = []\n",
    "for f in test_features:\n",
    "    tmp = sio.loadmat(os.path.join(test_features_path, f))\n",
    "    if 'faceCoordinatesUnwarped' in tmp:\n",
    "        features = tmp['faceCoordinatesUnwarped']\n",
    "    else:\n",
    "        features = tmp['faceCoordinates2']\n",
    "    test_features_list2.append(normalize(features)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78, 2)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_list2[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def euclideanـdistance(p1, p2):\n",
    "    tmp = ((p1[0] - p2[0]) * (p1[0] - p2[0]) + (p1[1] - p2[1]) * (p1[1] - p2[1]))\n",
    "    return math.sqrt(tmp)\n",
    "\n",
    "dist = []\n",
    "for i, p1 in enumerate(train_features_list2[0]):\n",
    "    for j, p2 in enumerate(train_features_list2[0]):\n",
    "        if j > i:\n",
    "            dist.append(euclideanـdistance(p1, p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 15.379674196243286 seconds\n",
      "Duration: 3.8808131217956543 seconds\n"
     ]
    }
   ],
   "source": [
    "euclideanـistance_train = []\n",
    "euclideanـdistance_test = []\n",
    "\n",
    "start_time = time.time()\n",
    "for im in train_features_list2:\n",
    "    dist = []\n",
    "    for i, p1 in enumerate(im):\n",
    "        for j, p2 in enumerate(im):\n",
    "            if j > i:\n",
    "                dist.append(euclideanـdistance(p1, p2))\n",
    "    euclideanـdistance_train.append(dist)    \n",
    "print(\"Duration: %s seconds\" % (time.time() - start_time)) \n",
    "   \n",
    "start_time = time.time()\n",
    "for im in test_features_list2:\n",
    "    dist = []\n",
    "    for i, p1 in enumerate(im):\n",
    "        for j, p2 in enumerate(im):\n",
    "            if j > i:\n",
    "                dist.append(euclideanـdistance(p1, p2))\n",
    "    euclideanـdistance_test.append(dist)    \n",
    "print(\"Duration: %s seconds\" % (time.time() - start_time)) \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3003"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(euclideanـdistance_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Duration : 4.265049934387207 seconds\n",
      "Train Accuracy Durtion: 0.2284531593322754 seconds\n",
      "Train Accuracy: 1.0\n",
      "Test Accuracy Duration: 0.10071396827697754 seconds\n",
      "Test Accuracy: 0.3532934131736527\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model = LDA()\n",
    "model.fit(np.array(euclideanـdistance_train), emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(np.array(euclideanـdistance_train), emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(np.array(euclideanـdistance_test), test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Duration : 0.8664889335632324 seconds\n",
      "Train Accuracy Durtion: 0.0017848014831542969 seconds\n",
      "Train Accuracy: 0.688344172086043\n",
      "Test Accuracy Duration: 0.07552528381347656 seconds\n",
      "Test Accuracy: 0.5469061876247505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "### euclideanـistance and pca and lda \n",
    "start_time = time.time()\n",
    "pca = PCA(n_components=100)\n",
    "reduced_features = pca.fit_transform(np.array(euclideanـdistance_train))\n",
    "model = LDA()\n",
    "model.fit(reduced_features, emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(reduced_features, emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "reduced_test = pca.transform(np.array(euclideanـdistance_test))\n",
    "accuracy = model.score(reduced_test, test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'someiterable' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-ee3e06595dc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'some.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msomeiterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'someiterable' is not defined"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with open('some.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(someiterable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('PCA_euclidean_features.csv', 'w') as csvfile:\n",
    "    fieldnames = ['*', 'last_']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    writer.writerow({'first_name': 'Baked', 'last_name': 'Beans'})\n",
    "    writer.writerow({'first_name': 'Lovely', 'last_name': 'Spam'})\n",
    "    writer.writerow({'first_name': 'Wonderful', 'last_name': 'Spam'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1999, 100)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "pd.DataFrame(reduced_features).to_csv(\"PCA_Euclidean_Features_Train.csv\")\n",
    "pd.DataFrame(reduced_test).to_csv(\"PCA_Euclidean_Features_Test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
