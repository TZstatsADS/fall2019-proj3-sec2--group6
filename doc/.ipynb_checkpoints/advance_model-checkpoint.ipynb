{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO:\n",
    "\n",
    "- correct the path for the pipeline\n",
    "- correct the pathes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.feature import hog\n",
    "from skimage.transform import resize\n",
    "from skimage import data, exposure\n",
    "from PIL import Image\n",
    "from sklearn.svm import SVC #SVM classifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_and_flatten(features):\n",
    "    features = features - np.min(features, axis=0, keepdims=True)\n",
    "    features = features / np.max(features, axis=0, keepdims=True)\n",
    "    return features.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'../data/train_set/label.csv' does not exist: b'../data/train_set/label.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-d249edaac2f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtrain_features_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'../data/train_set/train_points/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtest_features_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'../data/train_set/test_points/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../data/train_set/label.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mtrain_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_img_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\burningbamboo\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\burningbamboo\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\burningbamboo\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\burningbamboo\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\burningbamboo\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'../data/train_set/label.csv' does not exist: b'../data/train_set/label.csv'"
     ]
    }
   ],
   "source": [
    "resize_factor = (188, 250)\n",
    "\n",
    "train_img_path = '../data/train_set/train_images/'\n",
    "test_img_path = '../data/train_set/test_images/'\n",
    "train_features_path = '../data/train_set/train_points/'\n",
    "test_features_path = '../data/train_set/test_points/'\n",
    "labels = pd.read_csv('../data/train_set/label.csv')\n",
    "\n",
    "train_img = sorted(os.listdir(train_img_path))\n",
    "test_img = sorted(os.listdir(test_img_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRUE Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train emotion indexes\n",
    "train_indexes = []\n",
    "for img in train_img:\n",
    "    if img == \".DS_Store\":\n",
    "        continue\n",
    "    img = int(img[:4])\n",
    "    train_indexes.append(img - 1)\n",
    "    \n",
    "emotion_idx = labels[['emotion_idx']].loc[train_indexes].values[:, 0]\n",
    "\n",
    "### Test emotion indexes\n",
    "test_indexes = []\n",
    "for img in test_img:\n",
    "    if img == \".DS_Store\":\n",
    "        continue\n",
    "    img = int(img[:4])\n",
    "    test_indexes.append(img - 1)\n",
    "    \n",
    "test_emotion_idx = labels[['emotion_idx']].loc[test_indexes].values[:, 0]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEATURES\n",
    "#### HOGS, train_features, concat_train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 122.83936095237732 seconds ---\n",
      "--- 30.634644985198975 seconds ---\n"
     ]
    }
   ],
   "source": [
    "### extract HOG for train data set\n",
    "\n",
    "HOGs = []\n",
    "start_time = time.time()\n",
    "for img in train_img:\n",
    "    if img == '.DS_Store':\n",
    "        continue\n",
    "    img = np.asarray(Image.open(os.path.join(train_img_path, img)))\n",
    "    img = resize(img, resize_factor, anti_aliasing=False)\n",
    "    fetures_hog = hog(img, orientations=9, pixels_per_cell=(8, 8),\n",
    "                    cells_per_block=(3, 3), visualize=False, multichannel=True)\n",
    "    HOGs.append(fetures_hog)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))  \n",
    "\n",
    "### extract HOG for test data set\n",
    "\n",
    "HOGs_test = []\n",
    "start_time = time.time()\n",
    "for img in test_img:\n",
    "    if img == '.DS_Store':\n",
    "        continue\n",
    "    img = np.asarray(Image.open(os.path.join('../data/train_set/test_images/', img)))\n",
    "    img = resize(img, resize_factor, anti_aliasing=False)\n",
    "    fetures_hog = hog(img, orientations=9, pixels_per_cell=(8, 8),\n",
    "                    cells_per_block=(3, 3), visualize=False, multichannel=True)\n",
    "    HOGs_test.append(fetures_hog)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Point features for train dataset (NORMALIZED VERSION)\n",
    "train_features = sorted(os.listdir(train_features_path))\n",
    "train_features_list = []\n",
    "for f in train_features:\n",
    "    tmp = sio.loadmat(os.path.join(train_features_path, f))\n",
    "    if 'faceCoordinatesUnwarped' in tmp:\n",
    "        features = tmp['faceCoordinatesUnwarped']\n",
    "    else:\n",
    "        features = tmp['faceCoordinates2']\n",
    "    train_features_list.append(normalize_and_flatten(features))\n",
    "    \n",
    "### Point features for test dataset (NORMALIZED VERSION)\n",
    "test_features = sorted(os.listdir(test_features_path))\n",
    "test_features_list = []\n",
    "for f in test_features:\n",
    "    tmp = sio.loadmat(os.path.join(test_features_path, f))\n",
    "    if 'faceCoordinatesUnwarped' in tmp:\n",
    "        features = tmp['faceCoordinatesUnwarped']\n",
    "    else:\n",
    "        features = tmp['faceCoordinates2']\n",
    "    test_features_list.append(normalize_and_flatten(features))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 14.114684820175171 seconds ---\n",
      "--- 1.072826862335205 seconds ---\n"
     ]
    }
   ],
   "source": [
    "### Cancate of HOG and .mat features train\n",
    "start_time = time.time()\n",
    "pca = PCA(n_components=100)\n",
    "reduced_hogs = pca.fit_transform(np.array(HOGs)) \n",
    "train_np = np.array(train_features_list)\n",
    "concat_train_features = np.concatenate((reduced_hogs, train_np), axis=1)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "### test\n",
    "start_time = time.time()\n",
    "reduced_hogs_test = pca.transform(np.array(HOGs_test))\n",
    "test_np = np.array(test_features_list)\n",
    "concat_test_features = np.concatenate((reduced_hogs_test, test_np), axis=1)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASSIFIERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVM on HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Duration : 483.01157999038696 seconds\n",
      "Train Accuracy Durtion: 259.4689209461212 seconds\n",
      "Train Accuracy: 0.09404702351175588\n",
      "Test Accuracy Duration: 68.76027393341064 seconds\n",
      "Test Accuracy: 0.06786427145708583\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "linear_SVM = SVC(kernel=\"linear\", C=0.001)\n",
    "linear_SVM.fit(np.array(HOGs), emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = linear_SVM.score(np.array(HOGs), emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = linear_SVM.score(np.array(HOGs_test), test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVM on .mat features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Duration : 1.283754825592041 seconds\n",
      "Train Accuracy Durtion: 0.7464189529418945 seconds\n",
      "Train Accuracy: 0.05202601300650325\n",
      "Test Accuracy Duration: 0.19126558303833008 seconds\n",
      "Test Accuracy: 0.06187624750499002\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "linear_SVM = SVC(kernel=\"linear\", C=0.001)\n",
    "linear_SVM.fit(np.array(train_features_list), emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = linear_SVM.score(np.array(train_features_list), emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = linear_SVM.score(np.array(test_features_list), test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L SVM on concat features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Duration : 2.1608119010925293 seconds\n",
      "Train Accuracy Durtion: 1.254235029220581 seconds\n",
      "Train Accuracy: 0.07253626813406704\n",
      "Test Accuracy Duration: 0.31401801109313965 seconds\n",
      "Test Accuracy: 0.0658682634730539\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "linear_SVM = SVC(kernel=\"linear\", C=0.001)\n",
    "linear_SVM.fit(concat_train_features, emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = linear_SVM.score(concat_train_features, emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = linear_SVM.score(concat_test_features, test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L SVM on PCA HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Duration : 16.768794059753418 seconds\n",
      "Train Accuracy Durtion: 1.0205650329589844 seconds\n",
      "Train Accuracy: 0.39919959979989994\n",
      "Test Accuracy Duration: 0.7323682308197021 seconds\n",
      "Test Accuracy: 0.09580838323353294\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pca = PCA(n_components=200)\n",
    "reduced_features = pca.fit_transform(np.array(HOGs))\n",
    "linear_SVM = SVC(kernel=\"linear\", C=0.01)\n",
    "linear_SVM.fit(reduced_features, emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = linear_SVM.score(reduced_features, emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "reduced_test = pca.transform(np.array(HOGs_test))\n",
    "accuracy = linear_SVM.score(reduced_test, test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM on PCA HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Duration : 15.348134279251099 seconds\n",
      "Train Accuracy Durtion: 0.5401968955993652 seconds\n",
      "Train Accuracy: 0.9934967483741871\n",
      "Test Accuracy Duration: 0.5810391902923584 seconds\n",
      "Test Accuracy: 0.04590818363273453\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pca = PCA(n_components=100)\n",
    "reduced_features = pca.fit_transform(np.array(HOGs))\n",
    "model = SVC(gamma='auto', C=10)\n",
    "model.fit(reduced_features, emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(reduced_features, emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "reduced_test = pca.transform(np.array(HOGs_test))\n",
    "accuracy = model.score(reduced_test, test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM on .mat Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Duration : 0.7238380908966064 seconds\n",
      "Train Accuracy Durtion: 0.8284599781036377 seconds\n",
      "Train Accuracy: 0.6183091545772886\n",
      "Test Accuracy Duration: 0.1888411045074463 seconds\n",
      "Test Accuracy: 0.46506986027944114\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model = SVC(gamma='auto', C=1000)\n",
    "model.fit(np.array(train_features_list), emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(np.array(train_features_list), emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(np.array(test_features_list), test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM on concat features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Duration : 2.657891035079956 seconds\n",
      "Train Accuracy Durtion: 1.3531501293182373 seconds\n",
      "Train Accuracy: 0.8539269634817409\n",
      "Test Accuracy Duration: 0.36635494232177734 seconds\n",
      "Test Accuracy: 0.07584830339321358\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model = SVC(gamma='auto', C=10)\n",
    "model.fit(concat_train_features, emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(concat_train_features, emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(concat_test_features, test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForestClassifier on PCA HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Duration : 11.663228273391724 seconds\n",
      "Train Accuracy Durtion: 0.02125692367553711 seconds\n",
      "Train Accuracy: 0.5317658829414708\n",
      "Test Accuracy Duration: 0.337507963180542 seconds\n",
      "Test Accuracy: 0.06187624750499002\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pca = PCA(n_components=100)\n",
    "reduced_features = pca.fit_transform(np.array(HOGs))\n",
    "model = RandomForestClassifier(max_depth=5, n_estimators=50)\n",
    "model.fit(reduced_features, emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(reduced_features, emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "reduced_test = pca.transform(np.array(HOGs_test))\n",
    "accuracy = model.score(reduced_test, test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RFC on .mat features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Duration : 1.1041409969329834 seconds\n",
      "Train Accuracy Durtion: 0.030183076858520508 seconds\n",
      "Train Accuracy: 0.5157578789394698\n",
      "Test Accuracy Duration: 0.011514902114868164 seconds\n",
      "Test Accuracy: 0.3013972055888224\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model = RandomForestClassifier(max_depth=5, n_estimators=100)\n",
    "model.fit(np.array(train_features_list), emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(np.array(train_features_list), emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(np.array(test_features_list), test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RFC on cancate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Duration : 1.3742477893829346 seconds\n",
      "Train Accuracy Durtion: 0.05086398124694824 seconds\n",
      "Train Accuracy: 0.5327663831915957\n",
      "Test Accuracy Duration: 0.025084972381591797 seconds\n",
      "Test Accuracy: 0.3073852295409182\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model = RandomForestClassifier(max_depth=5, n_estimators=100)\n",
    "model.fit(concat_train_features, emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(concat_train_features, emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(concat_test_features, test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLPClassifier on pca HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Duration : 19.290612936019897 seconds\n",
      "Train Accuracy Durtion: 0.04608917236328125 seconds\n",
      "Train Accuracy: 0.9904952476238119\n",
      "Test Accuracy Duration: 0.9957513809204102 seconds\n",
      "Test Accuracy: 0.08183632734530938\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pca = PCA(n_components=100)\n",
    "reduced_features = pca.fit_transform(np.array(HOGs))\n",
    "model = MLPClassifier(hidden_layer_sizes=(300, 600, 300, 150, 60, 40))\n",
    "model.fit(reduced_features, emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(reduced_features, emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "reduced_test = pca.transform(np.array(HOGs_test))\n",
    "accuracy = model.score(reduced_test, test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP on .mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Duration : 3.587941884994507 seconds\n",
      "Train Accuracy Durtion: 0.0499267578125 seconds\n",
      "Train Accuracy: 0.1265632816408204\n",
      "Test Accuracy Duration: 0.011600971221923828 seconds\n",
      "Test Accuracy: 0.10778443113772455\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model = MLPClassifier(hidden_layer_sizes=(300, 600, 300, 150, 60, 40))\n",
    "model.fit(np.array(train_features_list), emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(np.array(train_features_list), emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(np.array(test_features_list), test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP on concat features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Duration : 5.981200933456421 seconds\n",
      "Train Accuracy Durtion: 0.04111790657043457 seconds\n",
      "Train Accuracy: 0.9959979989994997\n",
      "Test Accuracy Duration: 0.010310888290405273 seconds\n",
      "Test Accuracy: 0.11976047904191617\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model = MLPClassifier(hidden_layer_sizes=(300, 600, 300, 60, 40))\n",
    "model.fit(concat_train_features, emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(concat_train_features, emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(concat_test_features, test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
