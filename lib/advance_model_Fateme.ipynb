{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO:\n",
    "\n",
    "- correct the path for the pipeline\n",
    "- correct the pathes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.feature import hog\n",
    "from skimage.transform import resize\n",
    "from skimage import data, exposure\n",
    "from PIL import Image\n",
    "from sklearn.svm import SVC #SVM classifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from skimage.feature import local_binary_pattern\n",
    "from skimage.color import rgb2gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_and_flatten(features):\n",
    "    features = features - np.min(features, axis=0, keepdims=True)\n",
    "    features = features / np.max(features, axis=0, keepdims=True)\n",
    "    return features.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_factor = (188, 250)\n",
    "\n",
    "train_img_path = '../data/train_set/train_images/'\n",
    "test_img_path = '../data/train_set/test_images/'\n",
    "train_features_path = '../data/train_set/train_points/'\n",
    "test_features_path = '../data/train_set/test_points/'\n",
    "labels = pd.read_csv('../data/train_set/label.csv')\n",
    "\n",
    "train_img = sorted(os.listdir(train_img_path))\n",
    "test_img = sorted(os.listdir(test_img_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRUE Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train emotion indexes\n",
    "train_indexes = []\n",
    "for img in train_img:\n",
    "    if img == \".DS_Store\":\n",
    "        continue\n",
    "    img = int(img[:4])\n",
    "    train_indexes.append(img - 1)\n",
    "    \n",
    "emotion_idx = labels[['emotion_idx']].loc[train_indexes].values[:, 0]\n",
    "\n",
    "### Test emotion indexes\n",
    "test_indexes = []\n",
    "for img in test_img:\n",
    "    if img == \".DS_Store\":\n",
    "        continue\n",
    "    img = int(img[:4])\n",
    "    test_indexes.append(img - 1)\n",
    "    \n",
    "test_emotion_idx = labels[['emotion_idx']].loc[test_indexes].values[:, 0]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEATURES\n",
    "#### HOGS, train_features, concat_train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### extract HOG for train data set\n",
    "\n",
    "HOGs = []\n",
    "start_time = time.time()\n",
    "for img in train_img:\n",
    "    if img == '.DS_Store':\n",
    "        continue\n",
    "    img = np.asarray(Image.open(os.path.join(train_img_path, img)))\n",
    "    img = resize(img, resize_factor, anti_aliasing=False)\n",
    "    fetures_hog = hog(img, orientations=9, pixels_per_cell=(8, 8),\n",
    "                    cells_per_block=(3, 3), visualize=False, multichannel=True)\n",
    "    HOGs.append(fetures_hog)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))  \n",
    "\n",
    "### extract HOG for test data set\n",
    "\n",
    "HOGs_test = []\n",
    "start_time = time.time()\n",
    "for img in test_img:\n",
    "    if img == '.DS_Store':\n",
    "        continue\n",
    "    img = np.asarray(Image.open(os.path.join('../data/train_set/test_images/', img)))\n",
    "    img = resize(img, resize_factor, anti_aliasing=False)\n",
    "    fetures_hog = hog(img, orientations=9, pixels_per_cell=(8, 8),\n",
    "                    cells_per_block=(3, 3), visualize=False, multichannel=True)\n",
    "    HOGs_test.append(fetures_hog)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Point features for train dataset (NORMALIZED VERSION)\n",
    "train_features = sorted(os.listdir(train_features_path))\n",
    "train_features_list = []\n",
    "for f in train_features:\n",
    "    tmp = sio.loadmat(os.path.join(train_features_path, f))\n",
    "    if 'faceCoordinatesUnwarped' in tmp:\n",
    "        features = tmp['faceCoordinatesUnwarped']\n",
    "    else:\n",
    "        features = tmp['faceCoordinates2']\n",
    "    train_features_list.append(normalize_and_flatten(features))\n",
    "    \n",
    "### Point features for test dataset (NORMALIZED VERSION)\n",
    "test_features = sorted(os.listdir(test_features_path))\n",
    "test_features_list = []\n",
    "for f in test_features:\n",
    "    tmp = sio.loadmat(os.path.join(test_features_path, f))\n",
    "    if 'faceCoordinatesUnwarped' in tmp:\n",
    "        features = tmp['faceCoordinatesUnwarped']\n",
    "    else:\n",
    "        features = tmp['faceCoordinates2']\n",
    "    test_features_list.append(normalize_and_flatten(features))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_list[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cancate of HOG and .mat features train\n",
    "start_time = time.time()\n",
    "pca = PCA(n_components=100)\n",
    "reduced_hogs = pca.fit_transform(np.array(HOGs)) \n",
    "train_np = np.array(train_features_list)\n",
    "concat_train_features = np.concatenate((reduced_hogs, train_np), axis=1)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "### test\n",
    "start_time = time.time()\n",
    "reduced_hogs_test = pca.transform(np.array(HOGs_test))\n",
    "test_np = np.array(test_features_list)\n",
    "concat_test_features = np.concatenate((reduced_hogs_test, test_np), axis=1)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### extract LBP for train data set\n",
    "\n",
    "METHOD = 'uniform'\n",
    "radius = 3\n",
    "n_points = 8 * radius\n",
    "\n",
    "LBPs = []\n",
    "start_time = time.time()\n",
    "for img in train_img:\n",
    "    if img == '.DS_Store':\n",
    "        continue\n",
    "    img = np.asarray(Image.open(os.path.join(train_img_path, img)))\n",
    "    img = resize(img, resize_factor, anti_aliasing=False)\n",
    "    img = rgb2gray(img)\n",
    "#     print(img.shape)    \n",
    "    fetures_lbp = local_binary_pattern(img, n_points, radius, METHOD)\n",
    "    LBPs.append(fetures_lbp.flatten())\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))  \n",
    "\n",
    "### extract LBP for test data set\n",
    "\n",
    "LBPs_test = []\n",
    "start_time = time.time()\n",
    "for img in test_img:\n",
    "    if img == '.DS_Store':\n",
    "        continue\n",
    "    img = np.asarray(Image.open(os.path.join('../data/train_set/test_images/', img)))\n",
    "    img = resize(img, resize_factor, anti_aliasing=False)\n",
    "    img = rgb2gray(img)\n",
    "    fetures_lbp = local_binary_pattern(img, n_points, radius, METHOD)\n",
    "    LBPs_test.append(fetures_lbp.flatten())\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASSIFIERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVM on HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "linear_SVM = SVC(kernel=\"linear\", C=0.001)\n",
    "linear_SVM.fit(np.array(HOGs), emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = linear_SVM.score(np.array(HOGs), emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = linear_SVM.score(np.array(HOGs_test), test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVM on LBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "linear_SVM = SVC(kernel=\"linear\", C=0.001)\n",
    "linear_SVM.fit(np.array(LBPs), emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = linear_SVM.score(np.array(LBPs), emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = linear_SVM.score(np.array(LBPs_test), test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVM on PCA on LBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "pca = PCA(n_components=200)\n",
    "reduced_features = pca.fit_transform(np.array(LBPs))\n",
    "linear_SVM = SVC(kernel=\"linear\", C=0.0001)\n",
    "linear_SVM.fit(reduced_features, emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = linear_SVM.score(reduced_features, emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "reduced_test = pca.transform(np.array(LBPs_test))\n",
    "accuracy = linear_SVM.score(reduced_test, test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVM on .mat features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "linear_SVM = SVC(kernel=\"linear\", C=100)\n",
    "linear_SVM.fit(np.array(train_features_list), emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = linear_SVM.score(np.array(train_features_list), emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = linear_SVM.score(np.array(test_features_list), test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L SVM on concat features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "linear_SVM = SVC(kernel=\"linear\", C=0.5)\n",
    "linear_SVM.fit(concat_train_features, emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = linear_SVM.score(concat_train_features, emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = linear_SVM.score(concat_test_features, test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L SVM on PCA HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# pca = PCA(n_components=200)\n",
    "# reduced_features = pca.fit_transform(np.array(HOGs))\n",
    "linear_SVM = SVC(kernel=\"linear\", C=1)\n",
    "linear_SVM.fit(reduced_features, emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = linear_SVM.score(reduced_features, emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "reduced_test = pca.transform(np.array(HOGs_test))\n",
    "accuracy = linear_SVM.score(reduced_test, test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM on PCA HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "pca = PCA(n_components=100)\n",
    "reduced_features = pca.fit_transform(np.array(HOGs))\n",
    "model = SVC(gamma='auto', C=10)\n",
    "model.fit(reduced_features, emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(reduced_features, emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "reduced_test = pca.transform(np.array(HOGs_test))\n",
    "accuracy = model.score(reduced_test, test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "pca = PCA(n_components=200)\n",
    "reduced_features = pca.fit_transform(np.array(LBPs))\n",
    "model = SVC(gamma='auto', C=0.01)\n",
    "model.fit(reduced_features, emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(reduced_features, emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "reduced_test = pca.transform(np.array(LBPs_test))\n",
    "accuracy = model.score(reduced_test, test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM on .mat Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "model = SVC(gamma='auto', C=1000)\n",
    "model.fit(np.array(train_features_list), emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(np.array(train_features_list), emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(np.array(test_features_list), test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM on concat features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "model = SVC(gamma='auto', C=10)\n",
    "model.fit(concat_train_features, emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(concat_train_features, emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(concat_test_features, test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForestClassifier on PCA HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "pca = PCA(n_components=100)\n",
    "reduced_features = pca.fit_transform(np.array(HOGs))\n",
    "model = RandomForestClassifier(max_depth=5, n_estimators=50)\n",
    "model.fit(reduced_features, emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(reduced_features, emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "reduced_test = pca.transform(np.array(HOGs_test))\n",
    "accuracy = model.score(reduced_test, test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "pca = PCA(n_components=200)\n",
    "reduced_features = pca.fit_transform(np.array(LBPs))\n",
    "model = RandomForestClassifier(max_depth=5, n_estimators=100)\n",
    "model.fit(reduced_features, emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(reduced_features, emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "reduced_test = pca.transform(np.array(LBPs_test))\n",
    "accuracy = model.score(reduced_test, test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RFC on .mat features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "model = RandomForestClassifier(max_depth=5, n_estimators=100)\n",
    "model.fit(np.array(train_features_list), emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(np.array(train_features_list), emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(np.array(test_features_list), test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RFC on cancate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "model = RandomForestClassifier(max_depth=5, n_estimators=100)\n",
    "model.fit(concat_train_features, emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(concat_train_features, emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(concat_test_features, test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLPClassifier on pca HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "pca = PCA(n_components=100)\n",
    "reduced_features = pca.fit_transform(np.array(HOGs))\n",
    "model = MLPClassifier(hidden_layer_sizes=(300, 600, 300, 150, 60, 40))\n",
    "model.fit(reduced_features, emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(reduced_features, emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "reduced_test = pca.transform(np.array(HOGs_test))\n",
    "accuracy = model.score(reduced_test, test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "pca = PCA(n_components=200)\n",
    "reduced_features = pca.fit_transform(np.array(LBPs))\n",
    "model = MLPClassifier(hidden_layer_sizes=(300, 600, 300, 150, 60, 40))\n",
    "model.fit(reduced_features, emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(reduced_features, emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "reduced_test = pca.transform(np.array(LBPs_test))\n",
    "accuracy = model.score(reduced_test, test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP on .mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "model = MLPClassifier(hidden_layer_sizes=(300, 600, 300, 150, 60, 40))\n",
    "model.fit(np.array(train_features_list), emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(np.array(train_features_list), emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(np.array(test_features_list), test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP on concat features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "model = MLPClassifier(hidden_layer_sizes=(300, 600, 300, 60, 40))\n",
    "model.fit(concat_train_features, emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(concat_train_features, emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(concat_test_features, test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN on .mat features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "model.fit(np.array(train_features_list), emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(np.array(train_features_list), emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(np.array(test_features_list), test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN on pca HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "pca = PCA(n_components=100)\n",
    "reduced_features = pca.fit_transform(np.array(HOGs))\n",
    "model = KNeighborsClassifier(n_neighbors=10)\n",
    "model.fit(reduced_features, emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(reduced_features, emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "reduced_test = pca.transform(np.array(HOGs_test))\n",
    "accuracy = model.score(reduced_test, test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN on concate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "model.fit(concat_train_features, emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(concat_train_features, emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(concat_test_features, test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "start_time = time.time()\n",
    "model = AdaBoostClassifier(n_estimators=150)\n",
    "scores = cross_val_score(model, np.array(train_features_list), emotion_idx, cv=5)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "print(scores)\n",
    "\n",
    "# model.fit(np.array(train_features_list), emotion_idx)\n",
    "# print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "# start_time = time.time()\n",
    "# accuracy = model.score(np.array(train_features_list), emotion_idx)\n",
    "# print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "# print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "# start_time = time.time()\n",
    "# accuracy = model.score(np.array(test_features_list), test_emotion_idx)\n",
    "# print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "# print(\"Test Accuracy: {}\".format(accurac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import rotate\n",
    "from skimage.feature import local_binary_pattern\n",
    "from skimage import data\n",
    "from skimage.color import label2rgb\n",
    "\n",
    "# settings for LBP\n",
    "radius = 3\n",
    "n_points = 8 * radius\n",
    "\n",
    "\n",
    "def overlay_labels(image, lbp, labels):\n",
    "    mask = np.logical_or.reduce([lbp == each for each in labels])\n",
    "    return label2rgb(mask, image=image, bg_label=0, alpha=0.5)\n",
    "\n",
    "\n",
    "def highlight_bars(bars, indexes):\n",
    "    for i in indexes:\n",
    "        bars[i].set_facecolor('r')\n",
    "\n",
    "\n",
    "image = data.brick()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "start_time = time.time()\n",
    "model = LDA()\n",
    "model.fit(np.array(train_features_list), emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(np.array(train_features_list), emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(np.array(test_features_list), test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "pca = PCA(n_components=200)\n",
    "reduced_features = pca.fit_transform(np.array(LBPs))\n",
    "model = LDA()\n",
    "model.fit(reduced_features, emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(reduced_features, emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "reduced_test = pca.transform(np.array(LBPs_test))\n",
    "accuracy = model.score(reduced_test, test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### calculating the distance\n",
    "def normalize(features):\n",
    "    features = features - np.min(features, axis=0, keepdims=True)\n",
    "    features = features / np.max(features, axis=0, keepdims=True)\n",
    "    return features\n",
    "\n",
    "train_features = sorted(os.listdir(train_features_path))\n",
    "train_features_list2 = []\n",
    "for f in train_features:\n",
    "    tmp = sio.loadmat(os.path.join(train_features_path, f))\n",
    "    if 'faceCoordinatesUnwarped' in tmp:\n",
    "        features = tmp['faceCoordinatesUnwarped']\n",
    "    else:\n",
    "        features = tmp['faceCoordinates2']\n",
    "    train_features_list2.append(normalize(features))\n",
    "    \n",
    "test_features = sorted(os.listdir(test_features_path))\n",
    "test_features_list2 = []\n",
    "for f in test_features:\n",
    "    tmp = sio.loadmat(os.path.join(test_features_path, f))\n",
    "    if 'faceCoordinatesUnwarped' in tmp:\n",
    "        features = tmp['faceCoordinatesUnwarped']\n",
    "    else:\n",
    "        features = tmp['faceCoordinates2']\n",
    "    test_features_list2.append(normalize(features)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_list2[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def euclideanـdistance(p1, p2):\n",
    "    tmp = ((p1[0] - p2[0]) * (p1[0] - p2[0]) + (p1[1] - p2[1]) * (p1[1] - p2[1]))\n",
    "    return math.sqrt(tmp)\n",
    "\n",
    "dist = []\n",
    "for i, p1 in enumerate(train_features_list2[0]):\n",
    "    for j, p2 in enumerate(train_features_list2[0]):\n",
    "        if j > i:\n",
    "            dist.append(euclideanـdistance(p1, p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclideanـistance_train = []\n",
    "euclideanـdistance_test = []\n",
    "\n",
    "start_time = time.time()\n",
    "for im in train_features_list2:\n",
    "    dist = []\n",
    "    for i, p1 in enumerate(im):\n",
    "        for j, p2 in enumerate(im):\n",
    "            if j > i:\n",
    "                dist.append(euclideanـdistance(p1, p2))\n",
    "    euclideanـdistance_train.append(dist)    \n",
    "print(\"Duration: %s seconds\" % (time.time() - start_time)) \n",
    "   \n",
    "start_time = time.time()\n",
    "for im in test_features_list2:\n",
    "    dist = []\n",
    "    for i, p1 in enumerate(im):\n",
    "        for j, p2 in enumerate(im):\n",
    "            if j > i:\n",
    "                dist.append(euclideanـdistance(p1, p2))\n",
    "    euclideanـdistance_test.append(dist)    \n",
    "print(\"Duration: %s seconds\" % (time.time() - start_time)) \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(euclideanـdistance_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "model = LDA()\n",
    "model.fit(np.array(euclideanـdistance_train), emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(np.array(euclideanـdistance_train), emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(np.array(euclideanـdistance_test), test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### euclideanـistance and pca and lda \n",
    "start_time = time.time()\n",
    "pca = PCA(n_components=100)\n",
    "reduced_features = pca.fit_transform(np.array(euclideanـdistance_train))\n",
    "model = LDA()\n",
    "model.fit(reduced_features, emotion_idx)\n",
    "print(\"Train Duration : %s seconds\" % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = model.score(reduced_features, emotion_idx)\n",
    "print(\"Train Accuracy Durtion: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "start_time = time.time()\n",
    "reduced_test = pca.transform(np.array(euclideanـdistance_test))\n",
    "accuracy = model.score(reduced_test, test_emotion_idx)\n",
    "print(\"Test Accuracy Duration: %s seconds\" % (time.time() - start_time)) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "pd.DataFrame(reduced_features).to_csv(\"PCA_Euclidean_Features_Train.csv\")\n",
    "pd.DataFrame(reduced_test).to_csv(\"PCA_Euclidean_Features_Test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pd = pd.DataFrame(reduced_features)\n",
    "test_pd = pd.DataFrame(reduced_test)\n",
    "\n",
    "train_pd['emotion_idx'] = emotion_idx\n",
    "\n",
    "test_pd['emotion_idx'] = test_emotion_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pd.to_csv(\"PCA_Euclidean_Features_Train.csv\")\n",
    "test_pd.to_csv(\"PCA_Euclidean_Features_Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### distance is going to be the pairwise absolute value as a pca, and the lda "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### calculating the distance\n",
    "def normalize(features):\n",
    "    features = features - np.min(features, axis=0, keepdims=True)\n",
    "    features = features / np.max(features, axis=0, keepdims=True)\n",
    "    return features\n",
    "\n",
    "train_features = sorted(os.listdir(train_features_path))\n",
    "train_features_list2 = []\n",
    "for f in train_features:\n",
    "    tmp = sio.loadmat(os.path.join(train_features_path, f))\n",
    "    if 'faceCoordinatesUnwarped' in tmp:\n",
    "        features = tmp['faceCoordinatesUnwarped']\n",
    "    else:\n",
    "        features = tmp['faceCoordinates2']\n",
    "    train_features_list2.append(normalize(features))\n",
    "    \n",
    "test_features = sorted(os.listdir(test_features_path))\n",
    "test_features_list2 = []\n",
    "for f in test_features:\n",
    "    tmp = sio.loadmat(os.path.join(test_features_path, f))\n",
    "    if 'faceCoordinatesUnwarped' in tmp:\n",
    "        features = tmp['faceCoordinatesUnwarped']\n",
    "    else:\n",
    "        features = tmp['faceCoordinates2']\n",
    "    test_features_list2.append(normalize(features)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def pairwise_distance(p1, p2):\n",
    "    return abs(p1[0] - p2[0]), abs(p1[1] - p2[1])\n",
    "\n",
    "dist = []\n",
    "for i, p1 in enumerate(train_features_list2[0]):\n",
    "    for j, p2 in enumerate(train_features_list2[0]):\n",
    "        if j > i:\n",
    "            dist.append(pairwise_distance(p1, p2))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
